---
title: "Homework 03 Data Cleaning, Normal Distributions"
output:
  word_document: default
  html_document: default
  pdf_document: default
subtitle: Due by 11:59pm, Saturday, 2.10.24
author: S&DS 230/530/ENV 757
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


**(1) More on List Manipulation** *(18 points - 3 points each)*.

(1.1)  Make an object called `myList` that contains the following elements (in order):

*  The integers 1 through 10
*  A matrix with the integers 1 through 25 that has five rows, filled by row
*  A list that contains

      -  The text "latte"
      -  A vector with the text "taco" and "nan"
      -  A vector with the integers 1 through 7
      
You should be able to make this object in a single line of code.
```{r}
myList <- list(1:10, matrix(1:25, nrow = 5, byrow = TRUE), list("latte", c("taco", "nan"), 1:7))
```
*Use `[]`, `[[]]`, `[,]` notation to answer parts b) through f).*

(1.2)  Make an object called `ans1` that is the fourth row of the matrix contained in `myList`.

(1.3)  Make an object called `ans2` that is the sum of the 5th column of the matrix contained in `myList`.

(1.4)  Make an object called `ans3` that is the sum of EACH row of the matrix contained in `myList`  *(use the `apply()` function or check out `rowSums()`)*.

(1.5)  Make an object called `ans4` that is whichever single element of `myList` that you'd like to consume (yes, comedians, it has to be a food . . .).

(1.6)  Make an object called `ans5` that is the third element of the third element of `myList` converted to characters.

Get the results of each of your objects you created above (i.e. get them to show up in your knitted file by typing their names or putting the code line that creates each object in parentheses).

```{r}
#1.2
ans1 <- myList[[2]][4,]
ans1
#1.3
ans2 <- sum(myList[[2]][,5])
ans2
#1.4
ans3 <- rowSums(myList[[2]])
ans3
#1.5
ans4 <- myList[[3]][[1]]
ans4
#1.6
ans5 <- as.character(myList[[3]][[3]])
ans5
```

**(2) Normal Quantile Plots and the Binomial Distribution** *(20 points, 3 points each, part (2.5) is 5 points)*.

You may recall from your Intro Statistics course that a binomial distribution looks like a normal distribution if np > 10 and n(1-p) > 10  (i.e. as long as the average number of successes and failures are both larger than 10).  Recall that n is the number of trials, and p is the probability of success for each Bernoulli trial.   *As an example, flip a coin 30 times, count the number of heads.  n=30, p=.5, np = 15 > 10 and n(1-p) = 15 > 10, so the distribution should be approximately normal*.

You are going to make six normal quantile plots that simulate 100 random observations from binomial distributions with p = .2 and various values of n.

(2.1)  Install the `car` package.  This will allow you use the `qqPlot()` function.  Load this package.

(2.2)  Make a vector called `vec` that is powers of 10 for powers 0 through 5.  The one caveat, is that you need to use the`**` operator which reads as 'to the power of' *(i.e. `2**3` is 8).*

(2.3)  Use the `par()` function to set up your plot region to show 6 plots on a page.   The par argument you want is `mfrow = c(2,3)` which sets your plot region to have 2 rows and 3 columns.

(2.4)  Use the `rbinom()` function to generate 5 random binomial observations, each with 20 trials, and with p=0.8.  You may need to type `?rbinom` to get the syntax for this function.  Store the result in an object called `vec2`.

(2.5)  Write a loop that repeatedly creates a normal quantile plot for 100 random samples each from a binomial distribution with p=0.2 and n equal to the 6 values stored in `vec`.  A few plot details :
*  Use the `qqPlot` function.
*  Make the graph points red solid dots (`pch = 19`).
*  Make the boundary lines blue (use `col.lines`)
*  Make a main graph title that pastes the text "100 Binomial Samples, N =" to the corresponding value from `vec`.

```{r}
#2.1
library(car)
#2.2
vec <- 10**(0:5)
#2.3
par(mfrow = c(2,3))
#2.4
vec2 <- rbinom(5, 20, 0.8)
#2.5 CHECK THIS ONE
for (n in vec) {  
  qqPlot(rbinom(100, size = n, prob = 0.2), main = paste("100 Binomial Samples, N =", n), pch = 19, col.lines = "blue")
}
```

(2.6)  Take a look at the normal quantile plots.  For what value of n do the graphs seem to be approximately normally distributed?   Is this consistent with what you expect?   Write two complete sentences to answer these questions.

*For N being 1000, the graph seems to be normally distributed, as there is a smooth and continuous distribution rather than one that looks like a staircase for the ones below. Given p = 0.2 and np, n(1-p) needing to be greater than 10, I would expect anything above n=50 to have to look like a normal distribution, which n=100 sort of looks like but it's not smooth*
 
**(3) Favorite food and Data Cleaning** *(62 points.  Parts 3.2 through 3.5, 2 pts each, other values listed below)*.
 
This is data generated by former students of S&DS 230.  I simply asked "What is your favorite food?".  You can get the data [HERE](http://reuningscherer.net/S&DS230/data/food_230.csv).

Your goal is similar to what we did with the question "What animal would you like to be?" in Class 5 : clean this variable, make a barplot, and discuss the results.


(3.1) *(1 pnt)* Read in the data to a new object called `food`. 

(3.2)  Get a sense of the dataset - dimensions, variable names, look at the first few rows.

(3.3)  Convert `food` to a single vector that is just the first column (literally, replace `food` with `food$Food`).

(3.4)  Show the sorted unique values of `food`.  Calculate how many unique values exist in `food`.

```{r}
#3.1
food <- read.csv("Food_230.csv")
#3.2
dim(food)
names(food)
head(food)
#3.3
food <- food$Food
#3.4
sort(unique(food))
length(unique(food))
```
(3.5)  Write a couple of sentences about what data cleaning issues you notice amoung the unique values of `food`.

*There are some repeats with different capitalizations and some foods are have different formats, such as being plural or having a modifier. There are also some foods that are not actually foods, such as "delicious" and "anything with cheese". Some of the foods also contain multiple food items or are cuisines and not foods*

(3.6)  Cleaning Part I *(8 pts)*: Clean the data using the following steps (in order): 
Before proceeding to data cleaning, a quick reminder example of how to remove text before or after a particular word using the regular expression `.*` Note that `.` stands for 'any character' (other than a new line), and `*` stands for '0 or more times'.

Example : Find " have ", delete this AND everything preceeding or following:

```{r, eval = F}
#Deletes " have " and everything preceeding
gsub(".* have ", "", "Cats have personality")

#Deletes " have" and everything following
gsub(" have .*", "", "Cats have personality")

```
  

(3.6)  Cleaning Part I *(8 pts)*: Clean the data using the following steps (in order):

*  Convert data to lower case
*  Find " or " and remove this and anything that follows.
*  Find " and " and remove this and anything that follows.
*  Find " food" and remove this AND anything that follows.
*  Find " cuisine" and remove this AND anything that follows.
*  Remove all special characters and punctuation.
*  Remove trailing spaces at the end of text (use the `trimws()` function)

At each step, you'll probably want to check what unique values of `food` are left to make sure your functions are working correctly.  By the time you finish, you should have 156 unique levels.

Your final two lines of code should again show the sorted unique values of food and the current number of unique values.

```{r}
#3.6 HELP
food <- tolower(food)
food <- gsub(" or .*", "", food)
food <- gsub(" and .*", "", food)
food <- gsub(" food.*", "", food)
food <- gsub(" cuisine.*", "", food)
food <- gsub("[^0-9A-Za-z///' ]", "", food)
food <- trimws(food)
sort(unique(food))
length(unique(food))
```

(3.7) Cleaning Part II *(10 pts)*: A few quick random cleaning items:

Clean up the following types of food (in order) - one line of code per type of food.  In each case, deal with misspellings, modifiers ("shrimp curry" vs just "curry"), two words ('hot pot' instead of 'hotpot'), plurals, etc.

*  hotpot
*  curry
*  lasagna
*  noodles
*  cookies
*  chocolate
*  cheese
*  steak
*  sushi
*  fries (french, cajun, five guys' all just call 'fries')
*  ramen
*  tofu
*  burgers (of any kind)
*  soup
*  anything containing 'delicious' just call 'delicious'

When you're finished, you should have 130 unique values.

Your final two lines of code should again show the sorted unique values of food and the current number of unique values.

```{r}
#3.7
food <- gsub(".*(hot pot|hotpot).*", "hotpot", food)
food <- gsub(".*(curry|curries).*", "curry", food)
food <- gsub(".*(lasagna|lasagnas|lasagne).*", "lasagna", food)
food <- gsub(".*(noodles|noodle).*", "noodles", food)
food <- gsub(".*(cookies|cookie).*", "cookies", food)
food <- gsub(".*(chocolate|chocolates).*", "chocolate", food)
food <- gsub(".*cheese.*", "cheese", food)
food <- gsub(".*steak.*", "steak", food)
food <- gsub(".*sushi.*", "sushi", food)
food <- gsub(".*fries.*", "fries", food)
food <- gsub(".*ramen.*", "ramen", food)
food <- gsub(".*tofu.*", "tofu", food)
food <- gsub(".*(burgers|burger).*", "burgers", food)
food <- gsub(".*soup.*", "soup", food)
food <- gsub(".*delicious.*", "delicious", food)
sort(unique(food))
length(unique(food))

```

(3.8) Cleaning Part III *(8 pts)*: Cleaning types of cuisine.

Clean up the following types of cuisine (in order) - in this case, you'll want to make a vector called `searchvec` that contains the types of cuisine.  Then create a loop following the example in Class 5 to replace all the modifiers for each cuisine type so that you ultimately end up with cleaned up versions of each cuisine type.  Use not more than 5 lines of code.

The cuisine types (in order) are
*  asian
*  chinese
*  vietnamese
*  italian
*  indian
*  thai
*  mexican
*  brazilian
*  korean

(there are other types of cuisine, but they don't require cleaning).

When you're finished, you should have 120 unique values.

Your final two lines of code should again show the sorted unique values of food and the current number of unique values.

```{r}
#3.8
searchvec <- c("asian", "chinese", "vietnamese", "italian", "indian", "thai", "mexican", "brazilian", "korean")
for (i in searchvec) {
  food <- gsub(paste(".*", i, ".*", sep = ""), i, food)
}
sort(unique(food))
length(unique(food))
```
(3.9)  *(15 pts)*  Following the example from Class 05, display a dataframe of the sorted tabular results of `food` to see how many individuals prefer each kind of food.

From here on, the decisions of how to clean and combine categories are yours!  Any food that currently has a count of 3 or more should remain (you can add to these categories - for example, you could add 'lasagna' to 'italian' or to 'pasta').  All other levels should be recoded or incorporated into a 'miscellaneous' food category.  Points awarded based on thoughtfulness, effort, and quality/preciseness of your code.

Include your code below, and add comments where appropriate to describe the choices you make.  You should have no more than 40 levels by the time you finish.

Display a dataframe of the sorted tabular results of `food` to see how many individuals prefer each kind of food AGAIN after you've finished your coding.

```{r}
#3.9
foodframe <- data.frame(sort(table(food), decreasing = T))
foodframe
food_counts <- table(food)
popular_food <- names(food_counts[food_counts >= 3])
#sorting the chicken
food <- gsub(".*chicken.*", "chicken", food)
#grouping fried food
food <- gsub(".*fried.*", "fried food", food)
#sorting asian food
food <- gsub(".*(paneer|rice|postickers|hibachi|indonesian|turkish|vietnamese|dumplings|dal bhaat|tofu|persian).*", "asian", food)
#sorting african food
food <- gsub(".*(rice|african|ethiopian|tagine).*", "african", food)
#sorting american food
food <- gsub(".*(bbq|american|mac|hot dogs|chili|cornbread|pulled pork).*", "american", food)
#sorting european food
food <- gsub(".*(albanian|baguettes|crepes|lebanese|escargot|spaghetti|swedish).*", "european", food)
#sorting south american food
food <- gsub(".*(empanadas|guacamole|taco|farofa|flautas).*", "south american", food)
#everything not sorted is miscellaneous
categories <- c("chicken", "fried food", "asian", "african", "american", "european", "south american")
#setting everything that is not popular food or a category to miscellaneous
miscellaneous_food <- setdiff(unique(food), c(popular_food, categories))
food[food %in% miscellaneous_food] <- "miscellaneous"
foodframe <- data.frame(sort(table(food), decreasing = T))
foodframe
```

(3.10)  *(8 pts)*  Final steps and a plot:  You'll want to CAREFULLY follow the example in the code at the end of Class 05.

*  Use the `toTitleCase()` function from the package `tools` to convert food to title case.
*  Make an object called `finaltab` that is a table of your final vector `food`.
*  Calculate percents, rounded to the nearest integer, for each food type.  Save this as an object called `percents`.
*  Change the names of `finaltab` to include a space and then the percents followed by a "%" in curved parentheses.
*  Make a horizontal barplot of your final plot.   Choose a nice bar color, adjust the left margins as necessary, give a main title and label the horizontal axis.

```{r}
#3.10
library(tools)
food <- toTitleCase(food)
finaltab <- table(food)
percents <- round(finaltab/sum(finaltab)*100)
names(finaltab) <- paste(names(finaltab), " (", percents, "%)", sep = "")
par(mar = c(2, 6, 2, 2))
par(cex.axis = .5)

barplot(finaltab, horiz = T, col = "lightblue", las = 1, main = "Favorite Foods", xlab = "Percent of Students")

```

(3.11) *(3 pts)* In no more than three sentences, discuss your process and results.  Be sure to mention how many unique values of 'food' you started and ended with.  Any surprises?


*I started with 317 different unique values, and eventually came down to 34. The process started with removing duplicates by capitalizations and using some regular expressions for common foods. From there, I group some foods into the groups given and then based on their geographical cuisine group. I was suprised at the amount of data I was able to filter out while still getting a good understanding of the dataset and being able to present it in a much cleaner mamnner.*




